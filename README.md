# ğŸ“¹ Crime Capture TV : ë¬´ì¸ì í¬ ì´ìƒí–‰ë™ íƒì§€ ì‹œìŠ¤í…œ (AI part)

ë©”íƒ€ë²„ìŠ¤ ì•„ì¹´ë°ë¯¸ 9ì›” í”„ë¡œì íŠ¸

#### ğŸ¥ ì‹œì—° ì˜ìƒ ë³´ëŸ¬ê°€ê¸°([Click](https://www.youtube.com/watch?v=6DgZkKN7O5s))
#### ğŸ“™ ë°œí‘œìë£Œ ë³´ëŸ¬ê°€ê¸°([Click](https://github.com/crime-capture-tv/AI-server/blob/main/docs/Crime_capture_TV_presentation.pdf))

<img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/ace435a8-d4b3-4291-8627-dc59e052e55d" width="70%">

<br/>

# :family: íŒ€ì› ì†Œê°œ ë° ì—­í• 

**ê°œë°œê¸°ê°„: 2023.09.04 ~ 2023.09.27**

#### AI
| AI | AI | AI | AI |
|:--:|:--:|:--:|:--:|
| [ì •ë¯¼êµ](https://github.com/MinkyoJeong1) | [ê¹€ì¢…ë¯¼](https://github.com/jongminKims) | [ê¹€ì°¬ì˜](https://github.com/cykim1228) | [ìµœëˆˆì†”](https://github.com/choiary) |

#### Server
| server | server | server |
|:------:|:------:|:------:|
| [ë°•íƒœê·¼](https://github.com/taegeun-park0525) | [ê¹€ë‚˜ì˜](https://github.com/kny3037) | [ì´ì£¼ì›](https://github.com/juunewon) |

#### ê¸°íš
| ê¸°íš | ê¸°íš | ê¸°íš |
|:---:|:---:|:---:|
| [ê¹€ì˜ì‹](https://github.com/sikomar00) | [ì´ì„±ê· ](https://github.com/seongkyunlee) | [ì´ì§€ìˆ˜](https://github.com/geeeeesu) |

<br/>

### AI ì„¸ë¶€ ì—­í•  ë¶„ë‹´

<table>
    <tbody>
        <tr>
            <td><b>ì •ë¯¼êµ</b></td>
            <td>ë°ì´í„° ì „ì²˜ë¦¬ ë° VideoMAE model fine tuning</td>
        </tr>
        <tr>
            <td><b>ê¹€ì¢…ë¯¼</b></td>
            <td>Yolo-v8ì„ ì´ìš©í•œ humam detecting, ë¼ì¦ˆë² ë¦¬ íŒŒì´ CCTV ì œì‘</td>
        </tr>
        <tr>
            <td><b>ê¹€ì°¬ì˜</b></td>
            <td>ë°ì´í„° ì „ì²˜ë¦¬ ë° model serving</td>
        </tr>
        <tr>
            <td><b>ìµœëˆˆì†”</b></td>
            <td>ë°ì´í„° ì „ì²˜ë¦¬</td>
        </tr>
    </tbody>
</table>

<br/>

# ğŸ¤ ìœµí•© êµ¬ì¡°ë„

<img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/8331fda9-c20e-4ec2-b15c-c77a56ed916d" width="70%">

<br/>

# ğŸ’¡ í”„ë¡œì íŠ¸ ì†Œê°œ

**cctvë¥¼ ì´ìš©í•˜ì—¬ ë¬´ì¸ì í¬ ë‚´ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì´ìƒí–‰ë™ì„ ê°ì§€í•˜ëŠ” ì‹œìŠ¤í…œì„ ì œì‘**

<img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/f3c49525-2b6c-48aa-84a0-1ff56e1ddd0f" width="70%">

<img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/16740b64-0e7b-4a54-ae72-5bc4e7433f30" width="70%">

<br/>

# :scroll: ì£¼ìš” ë‚´ìš©

### 1. Prepare data set

- #### Source data
  
  [ai hub](https://www.aihub.or.kr/)ì— ìˆëŠ” [ì‹¤ë‚´(í¸ì˜ì , ë§¤ì¥) êµ¬ë§¤í–‰ë™ ë°ì´í„°](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=71549)ì™€
  [ì‹¤ë‚´(í¸ì˜ì , ë§¤ì¥) ì‚¬ëŒ ì´ìƒí–‰ë™ ë°ì´í„°](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71550)ë¥¼ ì´ìš©í•˜ì˜€ë‹¤.
  
- #### Preprocessing
  **1) ì›ë³¸ ë°ì´í„°ì—ì„œ ì›í•˜ëŠ” ë¶€ë¶„ í¸ì§‘**

     ì›ë³¸ ë°ì´í„°ëŠ” 1ë¶„ì§œë¦¬ ê¸´ ì˜ìƒì´ì—ˆìœ¼ë¯€ë¡œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” í–‰ë™ì„ í•˜ëŠ” êµ¬ê°„ë§Œ ì°¾ì•„ì„œ í¸ì§‘í•˜ì˜€ë‹¤. ì‚¬ëŒì˜ í–‰ë™ ë‹¹ ë¼ë²¨ì„ ë¶€ì—¬í•˜ì—¬ catch, put, insert, normalë¡œ ë¶„ë¥˜í•˜ì˜€ê³  ë¼ë²¨ë‹¹ ì•½ 300ê°œì˜ 2~5ì´ˆì§œë¦¬ ì˜ìƒì„ ì¤€ë¹„í•˜ì˜€ë‹¤.
     
  **2) í¸ì§‘ëœ ì˜ìƒì˜ í™”ì§ˆ ì¡°ì ˆ**

     ì›ë³¸ ì˜ìƒì˜ í¬ê¸°ëŠ” 1920 x 1080 /3 fps ì˜€ìœ¼ë‚˜ í¸ì§‘ê³¼ì •ì—ì„œ 1080 x 720 / 30 fpsë¡œ ë‚®ì•„ì¡Œë‹¤. ì—¬ê¸°ì„œ ì˜ìƒì„ í¬ë¡­í•˜ì˜€ì„ ë•Œ ì‚¬ëŒì˜ ëª¨ìŠµì´ ê½‰ ì°¨ë„ë¡ 640 x 480 ìœ¼ë¡œ ë‚®ì¶”ì—ˆë‹¤.
     
  **3) í¸ì§‘ëœ ì˜ìƒ ì¦ê°•**

     ì˜ìƒì´ ë¼ë²¨ë‹¹ ì•½ 300ê°œ ì •ë„ ì¤€ë¹„í•˜ì˜€ì§€ë§Œ ëª¨ë¸ í•™ìŠµì‹œì— ê³¼ì í•©ì´ ë°œìƒí•˜ì—¬ ë°ì´í„°ë¥¼ ì¦ê°• ì‹œì¼°ë‹¤.
     ì˜ìƒ ë°ì´í„° ì¦ê°•ì— ê´€í•œ ë¶€ë¶„ì€ ['3ì°¨ì› ì˜ë£Œ ì˜ìƒì˜ ì˜ì—­ ë¶„í• ì„ ìœ„í•œ íš¨ìœ¨ì ì¸ ë°ì´í„° ë³´ê°• ë°©ë²•'](https://koreascience.kr/article/JAKO202109156813970.pdf)ì´ë¼ëŠ” ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì˜€ìŒ.
	    
	    ì¦ê°• ë°©ë²•
	    Rotation  :  -10Â° ~ 10Â°
	    Brightness  :  -50 ~ 50
	    RGB  :  -30 ~ 30
       
  **4) ì‚¬ëŒì´ ìˆëŠ” ë¶€ë¶„ë§Œ Crop**

     ëª¨ë¸ì´ inputìœ¼ë¡œ ë°›ëŠ” ì˜ìƒì˜ ì‚¬ì´ì¦ˆê°€ 224 x 224 / 16 fps ì˜ ì˜ìƒì´ê¸° ë•Œë¬¸ì— ì²˜ìŒì—ëŠ” ê¸°ì¡´ ì˜ìƒì˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì—¬ì„œ ë„£ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ì§€ë§Œ
     ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•˜ì—¬ ì‚¬ëŒì´ ìˆëŠ” ë¶€ë¶„ì„ ì°¾ì•„ì„œ ì‚¬ëŒì„ ì¤‘ì‹¬ìœ¼ë¡œ 224 x 224 í¬ê¸°ë¡œ ìë¥¸ ë‹¤ìŒì— í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤. ì‚¬ëŒì„ ì°¾ê¸° ìœ„í•´ì„œ [YOLOv8](https://docs.ultralytics.com/)ì„ ì‚¬ìš©.

- #### Result
    
    <table>
        <tbody>
            <tr>
                <td align="center"><img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/f64fe674-3c0b-4e4a-ab35-cb05ee36f3c0" width="200" height="200"></td>
                <td align="center"><img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/a35db24a-c3f2-466a-b477-0dd3ebd0289d" width="200" height="200"></td>
                <td align="center"><img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/36da3032-8ad7-4f84-9c33-61a5cfa5b1cf" width="200" height="200"></td>
                <td align="center"><img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/ff9656e6-c790-42f1-b09d-d6efbff133ba" width="200" height="200"></td>
            </tr>
            <tr>
                <td align="center"><b>Source</b></td>
                <td align="center"><b>í¸ì§‘</b></td>
                <td align="center"><b>ì¦ê°•</b></td>
                <td align="center"><b>Crop</b></td>
            </tr>
            <tr>
                <td align="center">1920x1080 / 3fps</td>
                <td align="center">640x480 / 30fps</td>
                <td align="center">x4 or x6</td>
                <td align="center">224x224 / 16fps</td>
            </tr>
        </tbody>
    </table>

### 2. Model train

- #### VideoMAE

  [VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae) ëª¨ë¸ì€ [Vision Transformer(ViT)](https://huggingface.co/docs/transformers/model_doc/vit)ë¥¼ ì´ìš©í•œ [ImageMAE(ViTMAE)](https://huggingface.co/docs/transformers/model_doc/vit_mae) ëª¨ë¸ì„ ì˜ìƒìœ¼ë¡œ í™•ì¥í•œ ëª¨ë¸ì´ë‹¤. ì›ë³¸ ë…¼ë¬¸ì¸ ['VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training'](https://arxiv.org/abs/2203.12602)ê³¼ [github](https://github.com/MCG-NJU/VideoMAE).
  
- #### ViViT

  [ViViT](https://huggingface.co/docs/transformers/model_doc/vivit) ëª¨ë¸ì€ [Vision Transformer(ViT)](https://huggingface.co/docs/transformers/model_doc/vit)ì—ì„œ ì§ì ‘ í™•ì¥ëœ ì˜ìƒ ë¶„ë¥˜ ëª¨ë¸ì´ë‹¤. ë…¼ë¬¸[ 'ViViT: A Video Vision Transformer'](https://arxiv.org/abs/2103.15691)ê³¼ [github](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit).
	
	ê²°ë¡ ì ìœ¼ë¡  ViViT ëª¨ë¸ì´ VideoMAE ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•´ì„œ í…ŒìŠ¤íŠ¸ë§Œ ê±°ì¹˜ê³  ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒ.

- #### Fine turning

  ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ VideoMAEë¥¼ fine turningí•˜ëŠ” ë°©ë²•ì€ [ì´ ë…¸íŠ¸](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb)ë¥¼ ì°¸ê³ í•˜ì˜€ë‹¤. ViViT ëª¨ë¸ ë˜í•œ ê°™ì€ ì½”ë“œì—ì„œ VideoMAEImageProcessorì™€ VideoMAEForVideoClassificationë¥¼ VivitImageProcessorì™€ VivitForVideoClassificationë¡œ êµì²´í•˜ì—¬ í•™ìŠµí•˜ì˜€ë‹¤.
  
- #### Train result

  ì²˜ìŒ ì¦ê°•ê³¼ í¬ë¡­ì„ í•˜ì§€ ì•Šì€ ë°ì´í„° ì…‹ì„ ì´ìš©í•˜ì˜€ì„ ë•Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²°ê³¼ê°€ í•˜ë‚˜ì˜ ë¼ë²¨ë¡œ í¸í–¥ë˜ëŠ” ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆì—ˆë‹¤.
  
  ì´ëŠ” ëª¨ë¸ì´ ì‚¬ëŒì˜ í–‰ë™ì´ ì•„ë‹ˆë¼ ë°°ê²½ì„ í•™ìŠµí•˜ì—¬ ì´ëŸ° ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ì—¬ í¬ë¡­ ì‘ì—…ì„ ì§„í–‰í•˜ì˜€ë‹¤.
  
  ë˜í•œ fine turningëœ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ë•Œë¬¸ì— ê³¼ì í•©ì´ ì¼ì–´ë‚˜ ë°ì´í„°ë¥¼ ì¦ê°•ì‹œì¼œì„œ ëª¨ë¸ì„ í•™ìŠµ ì‹œì¼°ë‹¤. ì‹¤ì œë¡œ 2ë°°, 4ë°°, 6ë°° ì¦ê°•ëœ ë°ì´í„°ë¡œ í•™ìŠµì„ ì‹œí‚¨ ê²°ê³¼ ì¦ê°•ì„ í• ìˆ˜ë¡ ì •í™•ë„ê°€ ì¦ê°€í•˜ê³  í¸í–¥ì´ ì¤„ì–´ë“œëŠ” ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤.
  
- #### ì•„ì‰¬ìš´ ì 
   1) insertì™€ catchë¥¼ ì˜ êµ¬ë¶„í•˜ì§€ ëª»í–ˆë‹¤. insert ë°ì´í„° ì¤‘ ë¬¼ê±´ì„ ë„£ëŠ” ê³¼ì •ë§Œ ë“¤ì–´ê°€ì•¼ í•˜ëŠ”ë° ë¬¼ê±´ì„ ì§‘ì–´ì„œ ë„£ëŠ” ë¶€ë¶„ê¹Œì§€ í¬í•¨ëœ ì˜ìƒì´ ë§ì´ ìˆì—ˆê¸° ë•Œë¬¸ìœ¼ë¡œ ë³´ì—¬ì§.
   2) ì‚¬ëŒì´ ëŒë©´ put ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ê²½ìš°ê°€ ìˆì—ˆë‹¤. putì˜ìƒì—ì„œ ì‚¬ëŒì´ ë¬¼ê±´ì„ ë†“ê³  ë’¤ëŒì•„ ê°€ëŠ” ì¥ë©´ì´ í¬í•¨ë˜ì–´ ìˆì–´ì„œ ë¬¼ê±´ì„ ë†“ì§€ ì•Šì•„ë„ ëŒê¸°ë§Œ í•˜ë©´ putìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì—¬ì§.

- #### Result
    <table>
        <tbody>
            <tr>
                <td align="center"><img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/1b8a7b55-65e5-455b-8990-e681b4af893a" width="200" height="200"></td>
                <td align="center"><img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/e457d204-01ea-44f2-b098-13134b40f4f3" width="200" height="200"></td>
                <td align="center"><img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/8b295e33-0a69-49da-a2b8-538198eabd27" width="200" height="200"></td>
                <td align="center"><img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/428ab861-1214-43bd-9aaa-d961de8569f6" width="200" height="200"></td>
            </tr>
            <tr>
                <td align="center"><b>catch</b></td>
                <td align="center"><b>put</b></td>
                <td align="center"><b>insert</b></td>
                <td align="center"><b>walking</b></td>
            </tr>
        </tbody>
    </table>

### 3. Classification algorithm

<img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/90917a46-cfe2-4316-b0d0-8518c2f3e512" width="70%">

### 4. Raspberry Pi

- picameraì™€ socketserverë¥¼ ì‚¬ìš©í•˜ì—¬ cctvì˜ìƒì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì†¡ì¶œ.
- ë¶„ë¥˜ ì •í™•ë„ë¥¼ ìœ„í•˜ì—¬ Raspberry Pi 2ëŒ€ë¥¼ ì´ìš©í•˜ì—¬ ì˜ìƒì„ ë¶„ì„í•¨.
- pcì—ì„œ ë‘ ì˜ìƒì—ì„œ ì‚¬ëŒì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ë…¹í™”ë¥¼ ì§„í–‰í•˜ê³  ë…¹í™”ëœ ì˜ìƒì„ ë¶„ì„í•¨.

<img src="https://github.com/crime-capture-tv/AI-server/assets/141614581/2ec8a1e4-12c8-4838-bc32-dafb11b181ef" width="70%">

<br/>

# ğŸ›  ê¸°ìˆ  ìŠ¤íƒ

### - ì–¸ì–´
<img src="https://img.shields.io/badge/python-3776AB?style=for-the-badge&logo=python&logoColor=white">

### - ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬
 <img src="https://img.shields.io/badge/fastapi-009688?style=for-the-badge&logo=fastapi&logoColor=white"> <img src="https://img.shields.io/badge/pytorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white"> <img src="https://img.shields.io/badge/opencv-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white"> <img src="https://img.shields.io/badge/yolov8-00FFFF?style=for-the-badge&logo=yolo&logoColor=white">

### - ê°œë°œ íˆ´
<img src="https://img.shields.io/badge/VS code-2F80ED?style=for-the-badge&logo=VS code&logoColor=white"> <img src="https://img.shields.io/badge/Google Colab-F9AB00?style=for-the-badge&logo=Google Colab&logoColor=white">

### - í˜‘ì—… íˆ´
<img src="https://img.shields.io/badge/Github-181717?style=for-the-badge&logo=Github&logoColor=white"> <img src="https://img.shields.io/badge/Notion-000000?style=for-the-badge&logo=Notion&logoColor=white">

# ğŸ” ì°¸ê³ ìë£Œ

### Papers

1. [Tong, Z., Song, Y., Wang, J., & Wang, L. (2022, October 18). VideoMAE: Masked Autoencoders Are Data-Efficient Learners for Self-Supervised Video Pre-Training. Arxiv. https://arxiv.org/abs/2203.12602](https://arxiv.org/abs/2203.12602)
2. [Arnab, A., Dehghani, M., Heigold, G., Sun, C., LuÄiÄ‡, M., & Schmid, C. (2021, November 1). ViViT: A Video Vision Transformer. Arxiv. https://arxiv.org/abs/2103.15691](https://arxiv.org/abs/2103.15691)
3. [Park, S. (2021, October 28). An Efficient Data Augmentation for 3D Medical Image Segmentation. Koreascience. https://koreascience.kr/article/JAKO202109156813970.pdf](https://koreascience.kr/article/JAKO202109156813970.pdf)

### GitHub

1. [VideoMAE](https://github.com/MCG-NJU/VideoMAE)
2. [ViViT](https://github.com/rishikksh20/ViViT-pytorch)

